## Uniform Complexity for Text Generation 

This repository contains the code and datasets for the Uniform Complexity for Text Generation (UCTG) paper accepted at EMNLP 2023. 

UCTG proposes a challenge to test if humans and large language models like GPT-2 models (baseline and finetuned) are capable of maintaining linguistic complexity levels with respect to prompts.

Paper Link: https://aclanthology.org/2023.findings-emnlp.805/

Please add the following citation to your paper/presentation if you use the resources found in this repository:
```
Joseph Marvin Imperial and Harish Tayyar Madabushi. 2023. Uniform Complexity for Text Generation. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 12025â€“12046, Singapore. Association for Computational Linguistics.
```

## Contact

If you need any help reproducing the results, please don't hesitate to contact me through

**Joseph Marvin Imperial** <br/>
jmri20@bath.ac.uk <br/>
www.josephimperial.com 
