## Uniform Complexity for Text Generation 

This repository contains the code and datasets for the Uniform Complexity for Text Generation (UCTG) paper accepted at EMNLP 2023. 

UCTG proposes a challenge to test if humans and large language models like GPT-2 models (baseline and finetuned) are capable of maintaining linguistic complexity levels with respect to prompts.

Paper Link: https://arxiv.org/abs/2204.05185

Please add the following citation to your paper/presentation if you use the resources found in this repository:
```
Imperial, J. M. and Tayyar Madabushi, H. (2023). Uniform Complexity for Text Generation. arXiv preprint arXiv:2204.05185.
```

## Contact

If you need any help reproducing the results, please don't hesitate to contact me through

**Joseph Marvin Imperial** <br/>
jmri20@bath.ac.uk <br/>
www.josephimperial.com 
