## Uniform Complexity for Text Generation

This repository contains the code and datasets for testing if humans and GPT-2 models (baseline and finetuned) are capable of maintaining the linguistic complexity levels with respect to prompts.

Paper Link: https://arxiv.org/abs/2204.05185

Please add the following citation to your paper/presentation if you use the resources found in this repository:
```
Imperial, J. M. (2022). Uniform Complexity for Text Generation. arXiv preprint arXiv:2204.05185.
```

## Contact

If you need any help reproducing the results, please don't hesitate to contact me through

**Joseph Marvin Imperial** <br/>
jrimperial@national-u.edu.ph <br/>
www.josephimperial.com 
